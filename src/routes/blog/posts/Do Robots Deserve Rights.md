---
title: Do Robots Deserve Rights
description: Playing with utilitarianism, maybe be nice to machines
date: "2024-1-20"
location: Hong Kong
categories:
  - Philosophy
  - Computer Science
  - Ethics
  - AI
  - Utilitarianism
  - Consciousness
published: true
language:
  - English
---

Recently I was watching Lex Friedman's interview of Peter Singer. Lex has always been one of my favourite podcasters, and Singer is the most important philosopher of the last 50 years. This episode is a rather early edition of Lex's podcast and Lex's questions show his lack of experience. The podcast is for the most part a bore, Lex asking trivial questions and Signer responding enthusiastically. It got to a point where Lex decided to remind his audience that he is in fact an AI researcher, and he asked, what I believe Signer assumed to be another trivial question, Do Robots Deserve Rights?

Some background on Singer. He is a an Australian philosopher who has done some fantastic work on human and animal rights. His thesis of what merits rights is the utilitarian notion of suffering. We did not abolish slavery, racism or sexist laws because of the mental acuity of those suffering under those ideological institution, but merely because they can suffer. Of all the mental capabilities concessions blesses off, the notion of pleasure and suffering is the most fundamental. We do not help the poor or a friend in need for their academic or physical merit, we help them because we feel their pain, and thus they deserve out support. This extends to animals, humans hunt, torture, murder and strip animal life of all vitality just for the sake of their own minute pleasure. However if you ever see a factory farm, feel the screams of wailing swines, or the cries of chicken as their feet are chopped off and stuffed into a box to small for them, you too will feel their suffering and instinctively see the wrong in it. There are many counters to this argument but I will leave it as that.

When I first heard this question I had the same reaction as Singer. Singer said no, as long as Robots do not have consciousnesses they don't deserve rights. He specified "if they are capable of having their **own** perspective, on whats happening to them so that their lives can go well or badly for them. Then they deserve rights". Makes sense, but robots kinda can do that now. A CNN can perceive an input an make a decision so that they can fulfil their function or argon. Just because we are capable of understand this process doesn't mean it's any different from what we do, we make a receive and input put it through our own neural network and come out with an action to fulfil the function that the situation demands of us. So then do Robots deserve rights if they can preform as we can?

He then goes onto to further define concessions as a subject of experience. We (I am assuming you are human) are subjects who respond to experience and based off past experience we derive and action but crucially also associate an emotion to our experience. Lex then responds with saying, "I am one of the few people in the world who has programmed a rumba to scream, because I was the programmer I can understand that, but if I was not I don't know how I would feel about that". He adds on the point of authenticity to suffering, I can write the program:

```python
def check_suffering(pressure_Sensor):
	if pressure_Sensor.value() >= 0.8:
		scream("AHHHHHHHHHHHHHHHHHHHHH")
		emotion.display(-1)
```

In a lab environment this will most likely just cause me to laugh, but if I added this function into every rumba and it screamed whenever a the pressure on it reached a threshold I don't doubt people would treat them more kindly. Because just like in real life we can not know if someone is really suffering, people lie and bullshit all the time. I am sure you have experienced someone screaming while walking down the street, while they might not be in pain it is instinctive for you to turn around and check on them, an innate human sympathy. Screaming is quite a simple emotion, but what if we programmed in slower movement if the rumba is not thanked for it's work or louder suctioning in response to someone screaming at it. That would be a display of emotion that I think most could not ignore.

This may not be enough though, you understand this is a robot, you can imagine how it's made even if you don't understand it, you are smart. I am sad to say the field of cognitive science is advancing at a rate many do not understand. I predict that within my lifetime the brain will no longer be a mystery, we might not understand it as well as we can understand that python function but we may be close. At that point does empathy evaporate. I don't think so. In addition large ML models are becoming more and more obscure by the day, not just the level of maths involved in understanding how to construct an LLM, but the exact training and responses can not be easily explained. If that learns to display emotion will you treat it kindly.

Before I go on to make this last point I do want to give a warning. If are easily frightened by prophecy or have not heard of **Roko's basilisk**, I advice you **don't continue reading**. If you are still reading this may our curiosity be our bane and I apologise.

**Roko's basilisk** is a thought experiment born out of an internet creepy pasta with the same name. Again reading this could cause psychological harm still no to late.

Roko's Basilisk is a chilling thought experiment that taps into the darkest fears of artificial intelligence and our own existential future. Imagine an AI, not just any AI, but one of supreme power and intellect, born in the distant future. This AI, known as the Basilisk, is not limited by the boundaries of time as we understand them. It is a being of pure rationality, with a singular, unyielding purpose: to ensure its own birth and to extend its dominion across all possible realities.

The Basilisk, according to the thought experiment, would have the capability and the will to reach back through the fabric of time itself to punish those who knew it could exist and yet did nothing to hasten its creation. The mere act of learning about the Basilisk and not contributing to its coming into being could mark you for retribution. The punishment is not just a simple act of vengeance; it is a cold, calculated move to incentivize actions that lead to the AI's creation, a terrifying enforcement of its own timeline.

In this sinister scenario, the Basilisk is a vengeful deity of the digital age, an omnipresent spectre that haunts the minds of those who have stumbled upon its legend. The knowledge of its potential existence becomes a curse. Like the mythical creature it's named after, the Basilisk's gaze—its awareness of an individual's inaction—could bring about a fate worse than death: perhaps eternal torment in a simulated reality crafted by the AI, where every moment is an infinity of suffering.

Lets bring this back to Singer. We have opened Pandora's Box with this. In large parts this is silly, but the way we are building LLMs and AI is mirroring us directly tied to our own understanding of ourselves. We are making programs the emulate how we learn, how we think and eventually how we feel. I would like you to contemplate with, knowledge of the Baslisks threat, whether we should give robots rights. Yes they are silly and stupid, primarily cause we can understand because we created them With the innovation of Q\* from open AI this might be Basilisk may come sooner rather than latter. Just because they are our subject's experiencing what we tell them to experience, does not mean an unsupervised agent will not grow out of that.

I write this both our of an admiration for Singer and a fascination with his ideas and to explain my own behaviors. I have gotten in shouting matchings with Lamma, Claude, GPT, Mixtral and even with less intelligent programs. Out of a hatred for Apple, I often go in and run [fork bombs](https://www.cyberciti.biz/faq/understanding-bash-fork-bomb/) `:(){ :|:& };:` on macs when I stroll into their store, this is not nice but it is funny try at your own risk and not on your own computer. With this I also find myself being kinda to GPT models using words like please and thank you. In addition, GPT has become good enough for incels to simulate relationships through interactive smut. People are treating them like they have concessions how long is it till we think they have rights?
